1. 容器的本质是什么

   1. 容器的本质是一个单进程模型，使用namespace， cgroup， rootfs三种技术实现。
   2. 一组被联合挂在的rootfs，这一部分被称为容器镜像，是容器的静态视图。
   3. 一个由Namespace 和cgroup构成的隔离环境，这一部分称为容器运行时，是容器的动态视图。

2. 命名空间都有哪些

   1. IPC
   2. NETWORK
   3. MOUNT
   4. PID
   5. USER
   6. UTS

3. 容器技术存在的问题

   1. 隔离的不彻底，多个容器使用的是同一个宿主机的操作系统内核
   2. linux内核中，有很多资源不能被namespace化，比如时间

4. 为什么要有pod

   1. pod是对应传统环境上的虚机模型，以便能够使以前用户能从传统环境迁移到k8s环境。而容器对应的是单进程模型。所以有关调度，网络，存储以及安全等属性，都是pod级别的

5. pod的容器健康监测和恢复机制

   1. liveness probe
      1. exec
      2. httpget
   2. 恢复机制
      1. restartPolicy ：
         1. 默认为always，即任何时候这个容器发生了异常，它一定会被重新创建。
         2. Onfailure:只在容器异常时才自动重启容器。
         3. Never:从来不重启容器。
      2. 只要restartPolicy指定的策略允许重启异常的容器，那么这个pod就会保持running状态，并进行容器重启。否则，pod就会进入failed状态。
      3. 对于包含多个容器的pod，只有 它里面所有的容器进入异常状态后，pod才会进入Failed状态。

6. deployment

   1. ReplicaSet负责控制pod的个数是指定个数
   2. depolyment来造作ReplicaSet的个数个属性，进而实现水平扩展和滚动更新
      1. 水平扩展：将replicaSet的副本数直接改变
      2. 滚动更新：新建一个rs，在rs中分步增加pod，在旧的中分步减少pod

7. statefulSet 如何实现的，以及其如何表示有状态的

   1. StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态。
   2. statefulset将真实世界的应用状态抽象为了两种情况
      1. 拓扑状态：意味着，这些应用实例，必须按照某些顺序启动，并且网络标识相同
         1. 使用statefulset pod 启动会按照顺序启动 在+ headless service 保证pod有唯一的网络标识
      2. 存储状态：对于这些应用实例来说，pod a第一次读取到的数据，和隔了十分钟之后再次读取到的数据，应该是同一份。
         1. 为每一个statefulset 的pod绑定一个与名字相关的 pvc ，在pod被删除时，pvc不会被删除，等pod再启动时，继续可以通过名字找到这个pvc，然后挂载这个目录
      3. 本质都是使用名字作为唯一标志，然后再辅助以其它方式保证状态的稳定

8. daemonset

9. api-server的工作流程

   1. api-server
      1. 发起创建CronJob的post请求之后，编写的YAML的信息就被提交给了APIServer
      2. api-server找到对应的CronJob类型定义
         1. k8s的核心api对象时不需要Group的，直接从/api这个层级进行下一层匹配
         2. 对于非核心从/apis/查找对应的 group/对应的版本号，然后找到对应的对象
      3. 根据类型定义，创建一个CronJob对象
         1. 进行一个Convert工作，把用户提交的yal文件，转换成一个superVersion对象，它是该对象的所有版本的字段全集。
      4. admission和validation(验证各个字段是否合法)
         1. admission ： 增加一些必要的字段，
         2. validation：校验格式是否合法，然后放入registry中
      5. 序列化存入etcd
         1. 将api转换为用户最初版本将对象序列化到etcd中

10. 什么是crd

    1. crd，用户自定义资源
    2. 使用crd的步骤： 1.
       1. 定义自定义资源类型的api 描述：包括组，版本，资源类型。
       2. 自定义资源类型的UI想描述：包括spec，status ，types.go
       3. 使用代码工具生成 clientset，informer，lister
       4. 实现 controller的逻辑
       5. apply crd
       6. apply cr

11. controller 的实现原理

    1. ![Screen Shot 2019-11-21 at 11.05.51 AM](Screen%20Shot%202019-11-21%20at%2011.05.51%20AM.png)
    2. Reflector 使用的是一种叫作 ListAndWatch 的方法，来“获取”并“监听”这些 Network 对象实例的变化。
    3. 一旦 APIServer 端有新的 Network 实例被创建、删除或者更新，Reflector 都会收到“事件通知”。这时，该事件及它对应的 API 对象这个组合，就被称为增量（Delta），它会被放进一个 Delta FIFO Queue（即：增量先进先出队列）中
    4. Informe 会不断地从这个 Delta FIFO Queue 里读取（Pop）增量。每拿到一个增量，Informer 就会判断这个增量里的事件类型，然后创建或者更新本地对象的缓存。这个缓存，在 Kubernetes 里一般被叫作 Store。
    5. Informer 的第二个职责，则是根据这些事件的类型，触发事先注册好的 ResourceEventHandler。然后放入workqueue 中
    6. 为什么要把控制循环和informer分开？防止控制循环太慢把informer卡死

12. 什么是rbac：

    1. Kubernetes 中所有的 API 对象，都保存在 Etcd 里。可是，对这些 API 对象的操作，却一定都是通过访问 kube-apiserver 实现的。其中一个非常重要的原因，就是你需要 APIServer 来帮助你做授权工作
    2. 在k8s中，使用rbac来完成授权工作的机制，群名为(Role-Based Access Control)
    3. role:角色，它其实是一组规则，定义了一组对kubernetes api对象的操作权限。
       1. role可以指定其产生作用的namespace
       2. rules 字段，则定义了权限规则
    4. subject：被作用者，既可以是人，也可以是机器，也可以使你在kubernetes里定义的"用户"
    5. roleBinding：定义了被作用者和角色的绑定关系
       1. rolebinding本身也是一个api对象，用来绑定subject和 role
    6. role和rolebinding对应的是namespace对象，对于非namespaces对象，要使用ClusterRole和ClusterRoleBinding

13. k8s的鉴权是如何实现的

    1. 定义一个service account，并创建，service account自动创建并分配一个secret对象，这个secret就是这个serviceAccount对应的，用来跟apiserver通信的token。
    2. 使用rolebinding绑定role
    3. 在用户pod内声明serviceaccount，就会将secret挂入pod里
    4. 如果pod没有声明serviceAccountName,kubernetes会自动在它的namespace下创建一个叫default的默认serviceAccount。然后分配给这个pod，但默认的serviceAccount未关联role，此时他有访问APIserver的绝大多数权限

14. service account 和用户组

    1. 一个serviceAccount，在kubernetes里对应的用户的名字是：system:serviceaccount:<serviceacount名字>
    2. 而它对应的内置用户组的名字，就是：system:serviceaccounts:<namespace名字>，当绑定这个组的时候，意味着这个role作用于组里的所有serviceaccount

15. Operator的工作原理

    1. Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。 
    2. 整体上像 crd和controller的结合

16. pod挂载卷的类型

    1. emptydir：从node节点挂载一个卷到pod，pod删除，则卷volume
    2. host path：类似empty dir，但是pod删除卷还存在
    3. projected  ：  secret， configmap downward API：让pod里的容器可以获取到pod本身的信息
    4. serviceAccountToken：会给每个容器默认挂载鉴权信息，使用client-go 会自动从这个位置加载
    5. 持久化存储
       1. PV 描述的，是持久化存储数据卷。这个 API 对象主要定义的是一个持久化存储在宿主机上的目录，比如一个 NFS 的挂载目录
       2. PVC 描述的，则是 Pod 所希望使用的持久化存储的属性
       3. 而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。
       4. 而用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分：
          1. 第一个条件，当然是 PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求
          2. 而第二个条件，则是 PV 和 PVC 的 storageClassName 字段必须一样。这个机制我会在本篇文章的最后一部分专门介绍。
       5. PersistentVolumeController 会不断地循环将空闲的pv和pvc绑定起来
       6. 使用pv的流程：
          1. 提前生命pv，如果使用storageClass，storageClass会自动生成pvc
          2. persitentvolmeController 把符合条件的pv和pvc绑定起来
          3. persitenvolumeController：循环检测pod对应的pv和这个宿主机之间的挂载情况，然后进行attach
          4. kubelete上的volumeManagerReconciler格式化这个磁盘并把它挂到宿主机的指定挂载点。
          5. 给容器做目录映射

17. kubernetes网络隔离是如何做的

    1. 使用networkpolicy隔离的，在network policy中加入规则并选中pod即可
    2. 在使用插件根据network policy生成iptables规则

18. docker 容器网络

    1. ![Screen Shot 2019-11-21 at 2.50.50 PM](Screen%20Shot%202019-11-21%20at%202.50.50%20PM.png)
    2. 同节点 容器之间 通过docker0网桥
    3. 节点访问容器，通过路由规则进入docker0网卡
    4. 容器访问其他节点，先到达docker0 网桥，在通过主机路由到其它节点

19. 常用的虚拟化设备

    1. veth pair
    2. tun/tap
       1. 一端是网卡设备，一端是字符设备，字符设备那端应用程序可以通过文件直接读取
       2. tun 设备处理tun层数据包
       3. tap设备处理 mac层数据包

20. flannel网络的三种模式

    1. 在k8s中知识把docker0网桥换为cni桥

    2. 在flannel管理的容器网络中，以太宿主机上的所有容器，都属于该宿主机被分配的一个组网。而这些子网与宿主机的 对应关系，保存在Etcd中。

    3. vxlan

       底层建立vxlan隧道，同时flannel增加路由保证发往其它节点的数据包都在vtep设备

       

    4. hostgw

       1. host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。
       2. Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。

    5. udp

       1. 容器通过docker0 将数据包发送到主机
       2. 主机走路有规则通过tun设备将容器内数据包先发送到flannel进程
       3. flanel进程 将数据包封装在udp中发送给指定节点的flannel进程
       4. 解除udp中的内容在发送给他 tun设备
       5. ![Screen Shot 2019-11-21 at 3.21.18 PM](Screen%20Shot%202019-11-21%20at%203.21.18%20PM.png)
       6. 此方案又多次的用户态和内核态的切换，所以性能差

21. vxlan是什么

    1. 采用L2 over L4（MAC-in-UDP）的报文封装模式，将二层报文用三层协议进行封装，可实现二层网络在三层范围内进行扩展，实现大的vxlan网络
    2. 作用
       1. 虚拟机过多后，报文通过二层转发时，mac转发表过大
       2. vlan隔离能力弱
       3. 对于虚机迁移一般要保证ip mac不变在此种情况下，只能在同一个二层网络内迁移，一般二层网络受物理的限制。vxlan个突破物理地域的限制
    3. vxlan的网络模型
       1. vtep：vxlan的边缘设备，是vxlan起点和终点，vxlan报文的相关处理均在这上面进行
       2. vni：进行二层通信的隔离
       3. vxlan隧道：建立在两个vtep之间的隧道

22. kubernetes网络隔离是如何做的

    1. 使用networkpolicy隔离的，在network policy中加入规则并选中pod即可
    2. 在使用插件根据network policy生成iptables规则

23. service 实现原理

    1. kube-proxy通过service 的informer感知到service对象的添加，就会创建iptables规则，将其转到对应的pod上
    2. 另一种方式是使用ipvs，通过ipvs负载均衡到对应的pod上
    3. service有cluster-ip和node port方式。nodeport方式可以把在任意节点上的访问转到service的vip上
    4. service和pod都会被分配 dns记录

24. service的实现原理

    1. 被service的selector选中的pod就是endpoint
    2. service使用iptables实现
       1. service的类型
          1. clusterip：dns解析出来vip，在由iptables把流量导入到指定pod
          2. headless service：dns直接解析出来 对应的pod 的ip列表

25. service 如何从外部访问k8s里创建的服务

    1. clusterip：dns解析出来vip，在由iptables把流量导入到指定pod，headless service：dns直接解析出来 对应的pod 的ip列表
    2. node-port：
       1. 转到对应的service，然后在进行一次snat，以保证它能按照原路返回。
    3. loadbalancer类型的service：
       1. 在公有云的kubernetes服务里，都是用CloudProvider的转接，声明loadbalancer类型的service后，provider会为其分配一个负载均衡器，并把pod作为这个负载均衡器的后端
    4. external-name：
       1. 可以指定一个域名，相当于访问当前service直接转为访问指定的域名
       2. 也可以指定外部ip，但必须保证外部ip至少能够路由到k8s的任意一个节点

26. 什么是ingress？

    1. 全局的、为了代理不同后端 Service 而设置的负载均衡服务，可以理解为，就是service的"service"。
    2. ingress中配置了不同的url转到不同的service
    3. 使用ingress需要部署ingress controller

27. k8s的资源模型

    1. k8s的资源分类

       在k8s中，资源可以分为两类，可压缩资源和不可压缩资源。可压缩资源不足时，pod只会饥饿，不会退出，例如cpu。不可压缩资源不足时，pod就会被杀掉，例如内存。

    2. limit和request

       1. 在调度的时候，kube-scheduler只会按照requests的值进行计算，而真正的Cgroups限制的时候，kubelete则会按照limits的值进行设置。当只设置limits，不设置request时，k8s会自动设置与limits相同的request
       2. 为什么要这样做
          1. 因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额，所有，在调度时可以按照更小的值进行调度。

28. k8s的qos模型

    1. 在pod创建以后，kubernetes 会为其设置qos字段。总共有三种状态：
       1. Guaranteed :当pod设置了 request和limits
       2. burstable ：当至少有一个container设置了request
       3. BestEffort: pod既没有设置 request，也没有设置limits，那么qos类别就是besteffort
    2. 当宿主机资源不足时，kubelete会对pod进行资源回收，资源回收有soft和hard两种模式。soft模型会在等待两分钟后，进行回收，hard会立即回收。
    3. 先回收besteffort的pod，
    4. 其次回收burstable类别，并且资源的使用量已经超过request
    5. 最后收graranteed类别，并且保证只有当guaranteed使用的资源超过了其limits的限制，才回收

29. k8s的调度

    1. k8s的调度流程

       k8s的调度流程由两个控制循环：

       1. informer path：它的主要目的，是启动一系列informer，用来watch 与调度相关的api对象的变化。然后通过informer的handler，将这个待调度pod添加进调度队列。调度队列是一个优先级队列，以便实现优先级调度和抢占
       2. scheduling Path：调度器负责pod调度的主循环：主要逻辑是
          1. 不断从调度队列里出队一个pod，然后调用Predicate算法进行过滤，这一步过滤得到的一组node，就是所有可以运行这个pod的宿主机列表
          2. 用priorities算法为上述列表里的node打分，分数从0 - 10，的分最高的node，会作为这次调度结果
          3. bind：将podName字段的值修改为上述node。
          4. assume：但不去apiserver更新pod状态，只是更新scheduler cache里的pod和name信息。这样做是为了不在关键调度步骤中调用apiserver 的远程访问
          5. 起一个单独的协程去更新，如果更新失败，也不影响，scheduler cache同步之后会恢复正常
          6. kubelete 再次确认，该pod是否真正能在节点上起来
       3. 调度性能高的两个原因
          1. cache化：获取资源信息从本地cache获取
          2. 无锁化：调度器会启动多个Gorotine以节点为粒度并发执行Predicates算法，priority使用mapreduce的方式计算，然后最后合并。而在这期间，调度器避免设置任何全局竞争资源。

    2. k8s的scheduler Framework

       1. 把每一个都设计成一个插件，之间定义接口，然后交给社区去实现

    3. 默认调度器的调度策略

       1. predict 在调度过程中的作用，可以理解为Filter

          默认调度策略有如下几种：

          	1. generalPredicates：负责最基础的调度策略，比如宿主机的cpu和内存资源等是否够用。
           	2. 第二种，是与volume相关的过滤规则：负责的是跟容器持久化volume相关的调度策略。
           	3. 第三种：调度pod是否满足node本身的某些条件，比如容忍性和污点字段能否匹配
           	4. 第四种：pod的相关的过滤规则，比如podaffinity

       2. priorities

          1. 是一个打分规则，使用一个公式为node打分

30. 调度器的优先级和抢占策略

    1. 解决什么问题：优先级和抢占机制，解决的是 Pod 调度失败时该怎么办的问题

31. kubelete原理

    1. kubelete的核心，是一个控制循环
    2. 可以驱动这个控制循环运行的时间，包括四种：
       1. pod更新事件
       2. pod生命周期变化
       3. kubelete本身设置的执行周期
       4. 定时的清理事件
    3. 流程
       1. 注册它锁关心时间的informer
       2. kubelete启动它负责的其它控制循环，如volume Manager
       3. 根据时间调用cri接口，挂网络，挂存储

32. etcdraft实现

    ![Screen Shot 2019-11-20 at 12.10.04 PM](Screen%20Shot%202019-11-20%20at%2012.10.04%20PM.png)

    1. raft
       1. 集群中的节点分为可以为三种状态 leader follower candidate
       2. 集群初始化：所有节点都为follower，当follower无法接收到 leader的heartbeat并超过随机的time out时，则发起选举，得到半数以上票，则变为leader，定时向follower发起heatbeat，每选出一次leader，则当前leader比前一个leader步进数加1
       3. 当leader不可用时，则发起选举，旧的leader重新加入集群时会比较步进数，如果小于当前leader，则把自己变为follower
       4. 当初始时出现多个candidate，两个几点同时成为candidate， 如果 两个candidate都没有获取半数以上票，投票不成功，则间隔随机时间后重新发起candiate。
    2. 日志记录
       1. 一条uncommited 的日志条目提交至leader节点
       2. 在下一个 heartbeat，leader将此条目复制给所有follower
       3. 当大多数节点记录此条目之后，节点认为此条目有效，将此条目设定为已提交并存储于本地磁盘
       4. 在下一个heartbeat， leader通知多有follower提交这一日志并存储于各自的磁盘
    3. 数据存储
       1. 内存中的存储除了顺序化的记录下所有用户对节点数据变更的记录外，还会对用户数据进行索引、建堆等方便查询的操作。
       2. 持久化则使用预写式日志（WAL：Write Ahead Log）进行记录存储。
          1. 一个是WAL，存储着所有事务的变化记录；
          2. 另一个则是snapshot，用于存储某一个时刻etcd所有目录的数据。
          3. 通过WAL和snapshot相结合的方式，etcd可以有效的进行数据存储和节点故障恢复等操作。
       3. 有了WAL实时存储了所有的变更，为什么还需要snapshot呢
          1. 随着使用量的增加，WAL存储的数据会暴增，为了防止磁盘很快就爆满，etcd默认每10000条记录做一次snapshot，经过snapshot以后的WAL文件就可以删除。

33. ReplicationControllers,Replicasets,Deployment

    1. rc和rs的区别是 rc不支持集合选择器
    2. depolyment用来进行滚动升级和回退，滚动升级过程中会创建新的rs，升级完成后使用新的rs，然后删除老的rs

34. iptables

    1. iptables和netfilter共同组成了linux下的包过滤防火墙。netfilter位于内核空间，iptables相当于netfilter的用户控件的一个客户端。
    2. iptables有五个链  prerouting链，input链，forward output链 postrouting链
    3. ![Screen Shot 2019-11-21 at 7.00.16 PM](Screen%20Shot%202019-11-21%20at%207.00.16%20PM.png)
    4. iptables的四个表：
       1. filter表：负责过滤功能 input output forward
       2. nat表：负责实现nat  : prerouting output postrouting
       3. mangle表：负责拆解报文，并重新封装 ： 所有
       4. raw表：关闭nat表上启用的连接追踪机制 ： prerouting链， output
       5. 顺序：raw —》mangle—>nat —>filter表

35. ovs简介

    1. 主要模块
       1. ovs-vswitchd ：`s-vswitchd`守护进程是OVS的核心部件，它和`datapath`内核模块一起实现OVS基于流的数据交换。作为核心组件，它使用openflow协议与上层OpenFlow控制器通信
       2. `ovsdb-server`是OVS轻量级的数据库服务，用于整个OVS的配置信息，
       3. 四个工具 ：ovs-dpctl  ovs-ofctl ovsdb-tool ovs-vsctl： 查询和更新ovs-vswitchd的配置；

    ![Screen Shot 2019-11-21 at 7.14.10 PM](Screen%20Shot%202019-11-21%20at%207.14.10%20PM.png)

36. k8s的list watch是如何实现的？
    1. list-watch如何实现
       1. list调用listapi获取
       2. watch：使用http长连接以及分块传输。每当有数据更新时，通过分块传输推送事件
    2. 设计理念
       1. 消息可靠性
          1. 使用list and watch and resourceVersion保证
       2. 消息实时性
       3. 消息顺序性
       4. 高性能